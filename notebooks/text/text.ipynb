{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saashiv/miniconda3/envs/ERP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-26 01:42:45.286525: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-26 01:42:45.434867: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorboard/compat/__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m notf  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/home/saashiv/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfiftyone\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mfo\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1039\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/utils/import_utils.py:1121\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m   1120\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1121\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(module, name)\n\u001b[1;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/utils/import_utils.py:1120\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1119\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1120\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1121\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/utils/import_utils.py:1130\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_module\u001b[39m(\u001b[39mself\u001b[39m, module_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1129\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1131\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1133\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1134\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1135\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:42\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mactivations\u001b[39;00m \u001b[39mimport\u001b[39;00m ACT2FN\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     33\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     41\u001b[0m )\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n\u001b[1;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m     ModelOutput,\n\u001b[1;32m     46\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     replace_return_docstrings,\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/modeling_utils.py:88\u001b[0m\n\u001b[1;32m     85\u001b[0m XLA_DOWNCAST_BF16 \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mXLA_DOWNCAST_BF16\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mupper()\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m \u001b[39mimport\u001b[39;00m dispatch_model, infer_auto_device_map, init_empty_weights\n\u001b[1;32m     89\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mimport\u001b[39;00m add_hook_to_module\n\u001b[1;32m     90\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39maccelerate\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     91\u001b[0m         check_tied_parameters_on_same_device,\n\u001b[1;32m     92\u001b[0m         find_tied_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m         set_module_tensor_to_device,\n\u001b[1;32m     98\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.22.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39maccelerator\u001b[39;00m \u001b[39mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbig_modeling\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m skip_first_batches\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/accelerator.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mscheduler\u001b[39;00m \u001b[39mimport\u001b[39;00m AcceleratedScheduler\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mstate\u001b[39;00m \u001b[39mimport\u001b[39;00m AcceleratorState, GradientState, PartialState\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtracking\u001b[39;00m \u001b[39mimport\u001b[39;00m LOGGER_TYPE_TO_CLASS, GeneralTracker, filter_trackers\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     MODEL_NAME,\n\u001b[1;32m     44\u001b[0m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     wait_for_everyone,\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m FSDP_PYTORCH_VERSION\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/tracking.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mif\u001b[39;00m is_tensorboard_available():\n\u001b[1;32m     42\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m tensorboard\n\u001b[1;32m     44\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtensorboardX\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdel\u001b[39;00m LooseVersion\n\u001b[1;32m     10\u001b[0m \u001b[39mdel\u001b[39;00m tensorboard\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mwriter\u001b[39;00m \u001b[39mimport\u001b[39;00m FileWriter, SummaryWriter  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummary\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwriter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecord_writer\u001b[39;00m \u001b[39mimport\u001b[39;00m RecordWriter  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/torch/utils/tensorboard/writer.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msummary\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwriter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevent_file_writer\u001b[39;00m \u001b[39mimport\u001b[39;00m EventFileWriter\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_convert_np\u001b[39;00m \u001b[39mimport\u001b[39;00m make_np\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_embedding\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     make_mat,\n\u001b[1;32m     18\u001b[0m     make_sprite,\n\u001b[1;32m     19\u001b[0m     make_tsv,\n\u001b[1;32m     20\u001b[0m     write_pbtxt,\n\u001b[1;32m     21\u001b[0m     get_embedding_info,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_onnx_graph\u001b[39;00m \u001b[39mimport\u001b[39;00m load_onnx_graph\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_pytorch_graph\u001b[39;00m \u001b[39mimport\u001b[39;00m graph\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/torch/utils/tensorboard/_embedding.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m tf\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplugins\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprojector\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprojector_config_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m EmbeddingInfo\n\u001b[0;32m----> 9\u001b[0m _HAS_GFILE_JOIN \u001b[39m=\u001b[39m \u001b[39mhasattr\u001b[39m(tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39mgfile, \u001b[39m\"\u001b[39m\u001b[39mjoin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gfile_join\u001b[39m(a, b):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorboard/lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[0;34m(self, attr_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr_name):\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(load_once(\u001b[39mself\u001b[39;49m), attr_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorboard/lazy.py:97\u001b[0m, in \u001b[0;36m_memoize.<locals>.wrapper\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[1;32m     96\u001b[0m         \u001b[39mif\u001b[39;00m cache\u001b[39m.\u001b[39mget(arg, nothing) \u001b[39mis\u001b[39;00m nothing:\n\u001b[0;32m---> 97\u001b[0m             cache[arg] \u001b[39m=\u001b[39m f(arg)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m cache[arg]\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorboard/lazy.py:50\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.load_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m load_once\u001b[39m.\u001b[39mloading \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     module \u001b[39m=\u001b[39m load_fn()\n\u001b[1;32m     51\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     load_once\u001b[39m.\u001b[39mloading \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorboard/compat/__init__.py:45\u001b[0m, in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[39mreturn\u001b[39;00m tensorflow\n\u001b[1;32m     48\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/__init__.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     41\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tensorflow \u001b[39mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/python/eager/context.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclient\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tf_session\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m cancellation\n\u001b[0;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m execute\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m executor\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m monitoring\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_conversion_registry\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtf_export\u001b[39;00m \u001b[39mimport\u001b[39;00m tf_export\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m trace\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m \u001b[39mimport\u001b[39;00m trace_type\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocs\u001b[39;00m \u001b[39mimport\u001b[39;00m doc_controls\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtsl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_bfloat16\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/core/function/trace_type/__init__.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserialization\u001b[39;00m \u001b[39mimport\u001b[39;00m serialize\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserialization\u001b[39;00m \u001b[39mimport\u001b[39;00m SerializedTraceType\n\u001b[0;32m---> 35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m from_value\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m InternalCastContext\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m InternalPlaceholderContext\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:26\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrace_type\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m trace\n\u001b[0;32m---> 26\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInternalTracingContext\u001b[39;00m(trace\u001b[39m.\u001b[39mTracingContext):\n\u001b[1;32m     27\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Container for variables and flags shared across TraceType generation.\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, is_legacy_signature: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/abc.py:85\u001b[0m, in \u001b[0;36mABCMeta.__new__\u001b[0;34m(mcls, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__new__\u001b[39m(mcls, name, bases, namespace, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(mcls, name, bases, namespace, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     86\u001b[0m     _abc_init(\u001b[39mcls\u001b[39m)\n\u001b[1;32m     87\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers.generation import TFGenerationMixin\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading split 'train' to '/home/saashiv/fiftyone/open-images-v7/train' if necessary\n",
      "Ignoring invalid classes ['SUV', 'Police Van', 'Bridge', 'Road', 'Crosswalk', 'Sidewalk', 'Tunnel', 'Highway', 'Intersection', 'Walking', 'Shouting', 'Swimming', 'Jumping', 'Falling', 'Fighting', 'Waving', 'Mobile Phone', 'Water', 'Mountain', 'Beach', 'Lightning', 'Rain', 'Hail', 'One-Way Sign', 'Speed Limit Sign', 'Parking Sign', 'Exit Sign', 'No Entry Sign', 'School Crossing Sign', 'Pedestrian Sign', 'Crowd', 'Fire Hydrant', 'Gas Station', 'Fence', 'Statue', 'Playground', 'Electric Pole', 'Mailbox', 'Manhole', 'Public Transport Stop', 'Kiosk']\n",
      "You can view the available classes via `fiftyone.utils.openimages.get_classes()`\n",
      "Found 87 images, downloading the remaining 913\n",
      " 100% |███████████████████| 913/913 [13.5s elapsed, 0s remaining, 61.9 files/s]      \n",
      "Dataset info written to '/home/saashiv/fiftyone/open-images-v7/info.json'\n",
      "Loading existing dataset 'open-images-v7-train-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'validation' to '/home/saashiv/fiftyone/open-images-v7/validation' if necessary\n",
      "Ignoring invalid classes ['SUV', 'Police Van', 'Bridge', 'Road', 'Crosswalk', 'Sidewalk', 'Tunnel', 'Highway', 'Intersection', 'Walking', 'Shouting', 'Swimming', 'Jumping', 'Falling', 'Fighting', 'Waving', 'Mobile Phone', 'Water', 'Mountain', 'Beach', 'Lightning', 'Rain', 'Hail', 'One-Way Sign', 'Speed Limit Sign', 'Parking Sign', 'Exit Sign', 'No Entry Sign', 'School Crossing Sign', 'Pedestrian Sign', 'Crowd', 'Fire Hydrant', 'Gas Station', 'Fence', 'Statue', 'Playground', 'Electric Pole', 'Mailbox', 'Manhole', 'Public Transport Stop', 'Kiosk']\n",
      "You can view the available classes via `fiftyone.utils.openimages.get_classes()`\n",
      "Found 11 images, downloading the remaining 189\n",
      " 100% |███████████████████| 189/189 [3.8s elapsed, 0s remaining, 61.1 files/s]      \n",
      "Dataset info written to '/home/saashiv/fiftyone/open-images-v7/info.json'\n",
      "Loading existing dataset 'open-images-v7-validation-200'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Downloading split 'test' to '/home/saashiv/fiftyone/open-images-v7/test' if necessary\n",
      "Ignoring invalid classes ['SUV', 'Police Van', 'Bridge', 'Road', 'Crosswalk', 'Sidewalk', 'Tunnel', 'Highway', 'Intersection', 'Walking', 'Shouting', 'Swimming', 'Jumping', 'Falling', 'Fighting', 'Waving', 'Mobile Phone', 'Water', 'Mountain', 'Beach', 'Lightning', 'Rain', 'Hail', 'One-Way Sign', 'Speed Limit Sign', 'Parking Sign', 'Exit Sign', 'No Entry Sign', 'School Crossing Sign', 'Pedestrian Sign', 'Crowd', 'Fire Hydrant', 'Gas Station', 'Fence', 'Statue', 'Playground', 'Electric Pole', 'Mailbox', 'Manhole', 'Public Transport Stop', 'Kiosk']\n",
      "You can view the available classes via `fiftyone.utils.openimages.get_classes()`\n",
      "Found 43 images, downloading the remaining 157\n",
      " 100% |███████████████████| 157/157 [3.5s elapsed, 0s remaining, 56.1 files/s]      \n",
      "Dataset info written to '/home/saashiv/fiftyone/open-images-v7/info.json'\n",
      "Loading existing dataset 'open-images-v7-test-200'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "# Define classes of interest\n",
    "classes = [\n",
    "    \"Car\", \"Motorcycle\", \"Bus\", \"Boat\", \"Bicycle\", \"Truck\", \"SUV\", \"Taxi\", \"Police Van\", \"Helicopter\",\n",
    "    \"Bridge\", \"Building\", \"Road\", \"Crosswalk\", \"Sidewalk\", \"Tunnel\", \"Highway\", \"Intersection\",\n",
    "    \"Walking\", \"Shouting\", \"Swimming\", \"Jumping\", \"Falling\", \"Fighting\", \"Waving\",\n",
    "    \"Ladder\", \"Hammer\", \"Screwdriver\", \"Flashlight\", \"Backpack\", \"Camera\", \"Mobile Phone\", \"Laptop\",\n",
    "    \"Water\", \"Tree\", \"Mountain\", \"Beach\", \"Lightning\", \"Rain\", \"Hail\",\n",
    "    \"One-Way Sign\", \"Speed Limit Sign\", \"Parking Sign\", \"Exit Sign\", \"No Entry Sign\", \"School Crossing Sign\", \"Pedestrian Sign\",\n",
    "    \"Crowd\", \"Fire Hydrant\", \"Gas Station\", \"Bench\", \"Fence\", \"Statue\", \"Playground\", \"Electric Pole\", \"Mailbox\", \"Manhole\", \"Public Transport Stop\", \"Kiosk\"\n",
    "]\n",
    "\n",
    "# Load training set\n",
    "train_dataset = fo.zoo.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"train\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=classes,\n",
    "    max_samples=1000,  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Load validation set\n",
    "val_dataset = fo.zoo.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"validation\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=classes,\n",
    "    max_samples=200,  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Load test set\n",
    "test_dataset = fo.zoo.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    split=\"test\",\n",
    "    label_types=[\"detections\"],\n",
    "    classes=classes,\n",
    "    max_samples=200,  # Adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>desc</th>\n",
       "      <th>zip</th>\n",
       "      <th>title</th>\n",
       "      <th>timeStamp</th>\n",
       "      <th>twp</th>\n",
       "      <th>addr</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.297876</td>\n",
       "      <td>-75.581294</td>\n",
       "      <td>REINDEER CT &amp; DEAD END;  NEW HANOVER; Station ...</td>\n",
       "      <td>19525.0</td>\n",
       "      <td>EMS: BACK PAINS/INJURY</td>\n",
       "      <td>2015-12-10 17:10:52</td>\n",
       "      <td>NEW HANOVER</td>\n",
       "      <td>REINDEER CT &amp; DEAD END</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.258061</td>\n",
       "      <td>-75.264680</td>\n",
       "      <td>BRIAR PATH &amp; WHITEMARSH LN;  HATFIELD TOWNSHIP...</td>\n",
       "      <td>19446.0</td>\n",
       "      <td>EMS: DIABETIC EMERGENCY</td>\n",
       "      <td>2015-12-10 17:29:21</td>\n",
       "      <td>HATFIELD TOWNSHIP</td>\n",
       "      <td>BRIAR PATH &amp; WHITEMARSH LN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.121182</td>\n",
       "      <td>-75.351975</td>\n",
       "      <td>HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...</td>\n",
       "      <td>19401.0</td>\n",
       "      <td>Fire: GAS-ODOR/LEAK</td>\n",
       "      <td>2015-12-10 14:39:21</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "      <td>HAWS AVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.116153</td>\n",
       "      <td>-75.343513</td>\n",
       "      <td>AIRY ST &amp; SWEDE ST;  NORRISTOWN; Station 308A;...</td>\n",
       "      <td>19401.0</td>\n",
       "      <td>EMS: CARDIAC EMERGENCY</td>\n",
       "      <td>2015-12-10 16:47:36</td>\n",
       "      <td>NORRISTOWN</td>\n",
       "      <td>AIRY ST &amp; SWEDE ST</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.251492</td>\n",
       "      <td>-75.603350</td>\n",
       "      <td>CHERRYWOOD CT &amp; DEAD END;  LOWER POTTSGROVE; S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EMS: DIZZINESS</td>\n",
       "      <td>2015-12-10 16:56:52</td>\n",
       "      <td>LOWER POTTSGROVE</td>\n",
       "      <td>CHERRYWOOD CT &amp; DEAD END</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40.253473</td>\n",
       "      <td>-75.283245</td>\n",
       "      <td>CANNON AVE &amp; W 9TH ST;  LANSDALE; Station 345;...</td>\n",
       "      <td>19446.0</td>\n",
       "      <td>EMS: HEAD INJURY</td>\n",
       "      <td>2015-12-10 15:39:04</td>\n",
       "      <td>LANSDALE</td>\n",
       "      <td>CANNON AVE &amp; W 9TH ST</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40.182111</td>\n",
       "      <td>-75.127795</td>\n",
       "      <td>LAUREL AVE &amp; OAKDALE AVE;  HORSHAM; Station 35...</td>\n",
       "      <td>19044.0</td>\n",
       "      <td>EMS: NAUSEA/VOMITING</td>\n",
       "      <td>2015-12-10 16:46:48</td>\n",
       "      <td>HORSHAM</td>\n",
       "      <td>LAUREL AVE &amp; OAKDALE AVE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40.217286</td>\n",
       "      <td>-75.405182</td>\n",
       "      <td>COLLEGEVILLE RD &amp; LYWISKI RD;  SKIPPACK; Stati...</td>\n",
       "      <td>19426.0</td>\n",
       "      <td>EMS: RESPIRATORY EMERGENCY</td>\n",
       "      <td>2015-12-10 16:17:05</td>\n",
       "      <td>SKIPPACK</td>\n",
       "      <td>COLLEGEVILLE RD &amp; LYWISKI RD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40.289027</td>\n",
       "      <td>-75.399590</td>\n",
       "      <td>MAIN ST &amp; OLD SUMNEYTOWN PIKE;  LOWER SALFORD;...</td>\n",
       "      <td>19438.0</td>\n",
       "      <td>EMS: SYNCOPAL EPISODE</td>\n",
       "      <td>2015-12-10 16:51:42</td>\n",
       "      <td>LOWER SALFORD</td>\n",
       "      <td>MAIN ST &amp; OLD SUMNEYTOWN PIKE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40.102398</td>\n",
       "      <td>-75.291458</td>\n",
       "      <td>BLUEROUTE  &amp; RAMP I476 NB TO CHEMICAL RD; PLYM...</td>\n",
       "      <td>19462.0</td>\n",
       "      <td>Traffic: VEHICLE ACCIDENT -</td>\n",
       "      <td>2015-12-10 17:35:41</td>\n",
       "      <td>PLYMOUTH</td>\n",
       "      <td>BLUEROUTE  &amp; RAMP I476 NB TO CHEMICAL RD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40.231990</td>\n",
       "      <td>-75.251891</td>\n",
       "      <td>RT202 PKWY &amp; KNAPP RD; MONTGOMERY; 2015-12-10 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Traffic: VEHICLE ACCIDENT -</td>\n",
       "      <td>2015-12-10 17:33:50</td>\n",
       "      <td>MONTGOMERY</td>\n",
       "      <td>RT202 PKWY &amp; KNAPP RD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40.084161</td>\n",
       "      <td>-75.308386</td>\n",
       "      <td>BROOK RD &amp; COLWELL LN; PLYMOUTH; 2015-12-10 @ ...</td>\n",
       "      <td>19428.0</td>\n",
       "      <td>Traffic: VEHICLE ACCIDENT -</td>\n",
       "      <td>2015-12-10 16:32:10</td>\n",
       "      <td>PLYMOUTH</td>\n",
       "      <td>BROOK RD &amp; COLWELL LN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40.174131</td>\n",
       "      <td>-75.098491</td>\n",
       "      <td>BYBERRY AVE &amp; S WARMINSTER RD; UPPER MORELAND;...</td>\n",
       "      <td>19040.0</td>\n",
       "      <td>Traffic: VEHICLE ACCIDENT -</td>\n",
       "      <td>2015-12-10 17:15:49</td>\n",
       "      <td>UPPER MORELAND</td>\n",
       "      <td>BYBERRY AVE &amp; S WARMINSTER RD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40.062974</td>\n",
       "      <td>-75.135914</td>\n",
       "      <td>OLD YORK RD &amp; VALLEY RD; CHELTENHAM; 2015-12-1...</td>\n",
       "      <td>19027.0</td>\n",
       "      <td>Traffic: VEHICLE ACCIDENT -</td>\n",
       "      <td>2015-12-10 17:12:47</td>\n",
       "      <td>CHELTENHAM</td>\n",
       "      <td>OLD YORK RD &amp; VALLEY RD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40.097222</td>\n",
       "      <td>-75.376195</td>\n",
       "      <td>SCHUYLKILL EXPY &amp; CROTON RD UNDERPASS; UPPER M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Traffic: VEHICLE ACCIDENT -</td>\n",
       "      <td>2015-12-10 17:09:49</td>\n",
       "      <td>UPPER MERION</td>\n",
       "      <td>SCHUYLKILL EXPY &amp; CROTON RD UNDERPASS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lat        lng                                               desc  \\\n",
       "0   40.297876 -75.581294  REINDEER CT & DEAD END;  NEW HANOVER; Station ...   \n",
       "1   40.258061 -75.264680  BRIAR PATH & WHITEMARSH LN;  HATFIELD TOWNSHIP...   \n",
       "2   40.121182 -75.351975  HAWS AVE; NORRISTOWN; 2015-12-10 @ 14:39:21-St...   \n",
       "3   40.116153 -75.343513  AIRY ST & SWEDE ST;  NORRISTOWN; Station 308A;...   \n",
       "4   40.251492 -75.603350  CHERRYWOOD CT & DEAD END;  LOWER POTTSGROVE; S...   \n",
       "5   40.253473 -75.283245  CANNON AVE & W 9TH ST;  LANSDALE; Station 345;...   \n",
       "6   40.182111 -75.127795  LAUREL AVE & OAKDALE AVE;  HORSHAM; Station 35...   \n",
       "7   40.217286 -75.405182  COLLEGEVILLE RD & LYWISKI RD;  SKIPPACK; Stati...   \n",
       "8   40.289027 -75.399590  MAIN ST & OLD SUMNEYTOWN PIKE;  LOWER SALFORD;...   \n",
       "9   40.102398 -75.291458  BLUEROUTE  & RAMP I476 NB TO CHEMICAL RD; PLYM...   \n",
       "10  40.231990 -75.251891  RT202 PKWY & KNAPP RD; MONTGOMERY; 2015-12-10 ...   \n",
       "11  40.084161 -75.308386  BROOK RD & COLWELL LN; PLYMOUTH; 2015-12-10 @ ...   \n",
       "12  40.174131 -75.098491  BYBERRY AVE & S WARMINSTER RD; UPPER MORELAND;...   \n",
       "13  40.062974 -75.135914  OLD YORK RD & VALLEY RD; CHELTENHAM; 2015-12-1...   \n",
       "14  40.097222 -75.376195  SCHUYLKILL EXPY & CROTON RD UNDERPASS; UPPER M...   \n",
       "\n",
       "        zip                        title            timeStamp  \\\n",
       "0   19525.0       EMS: BACK PAINS/INJURY  2015-12-10 17:10:52   \n",
       "1   19446.0      EMS: DIABETIC EMERGENCY  2015-12-10 17:29:21   \n",
       "2   19401.0          Fire: GAS-ODOR/LEAK  2015-12-10 14:39:21   \n",
       "3   19401.0       EMS: CARDIAC EMERGENCY  2015-12-10 16:47:36   \n",
       "4       NaN               EMS: DIZZINESS  2015-12-10 16:56:52   \n",
       "5   19446.0             EMS: HEAD INJURY  2015-12-10 15:39:04   \n",
       "6   19044.0         EMS: NAUSEA/VOMITING  2015-12-10 16:46:48   \n",
       "7   19426.0   EMS: RESPIRATORY EMERGENCY  2015-12-10 16:17:05   \n",
       "8   19438.0        EMS: SYNCOPAL EPISODE  2015-12-10 16:51:42   \n",
       "9   19462.0  Traffic: VEHICLE ACCIDENT -  2015-12-10 17:35:41   \n",
       "10      NaN  Traffic: VEHICLE ACCIDENT -  2015-12-10 17:33:50   \n",
       "11  19428.0  Traffic: VEHICLE ACCIDENT -  2015-12-10 16:32:10   \n",
       "12  19040.0  Traffic: VEHICLE ACCIDENT -  2015-12-10 17:15:49   \n",
       "13  19027.0  Traffic: VEHICLE ACCIDENT -  2015-12-10 17:12:47   \n",
       "14      NaN  Traffic: VEHICLE ACCIDENT -  2015-12-10 17:09:49   \n",
       "\n",
       "                  twp                                      addr  e  \n",
       "0         NEW HANOVER                    REINDEER CT & DEAD END  1  \n",
       "1   HATFIELD TOWNSHIP                BRIAR PATH & WHITEMARSH LN  1  \n",
       "2          NORRISTOWN                                  HAWS AVE  1  \n",
       "3          NORRISTOWN                        AIRY ST & SWEDE ST  1  \n",
       "4    LOWER POTTSGROVE                  CHERRYWOOD CT & DEAD END  1  \n",
       "5            LANSDALE                     CANNON AVE & W 9TH ST  1  \n",
       "6             HORSHAM                  LAUREL AVE & OAKDALE AVE  1  \n",
       "7            SKIPPACK              COLLEGEVILLE RD & LYWISKI RD  1  \n",
       "8       LOWER SALFORD             MAIN ST & OLD SUMNEYTOWN PIKE  1  \n",
       "9            PLYMOUTH  BLUEROUTE  & RAMP I476 NB TO CHEMICAL RD  1  \n",
       "10         MONTGOMERY                     RT202 PKWY & KNAPP RD  1  \n",
       "11           PLYMOUTH                     BROOK RD & COLWELL LN  1  \n",
       "12     UPPER MORELAND             BYBERRY AVE & S WARMINSTER RD  1  \n",
       "13         CHELTENHAM                   OLD YORK RD & VALLEY RD  1  \n",
       "14       UPPER MERION     SCHUYLKILL EXPY & CROTON RD UNDERPASS  1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'/home/saashiv/Code/EMR/data/text_data/raw/911.csv')\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "Traffic: VEHICLE ACCIDENT -    148372\n",
      "Traffic: DISABLED VEHICLE -     47909\n",
      "Fire: FIRE ALARM                38336\n",
      "EMS: FALL VICTIM                34676\n",
      "EMS: RESPIRATORY EMERGENCY      34248\n",
      "                                ...  \n",
      "EMS: DISABLED VEHICLE               1\n",
      "Fire: PRISONER IN CUSTODY           1\n",
      "Fire: GENERAL WEAKNESS              1\n",
      "Fire: SUSPICIOUS                    1\n",
      "Fire: BARRICADED SUBJECT            1\n",
      "Name: count, Length: 148, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the occurrences of each unique value in the 'title' column\n",
    "class_distribution = df['title'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced DataFrame: title\n",
      "EMS: BACK PAINS/INJURY                  1000\n",
      "EMS: DIABETIC EMERGENCY                 1000\n",
      "EMS: ABDOMINAL PAINS                    1000\n",
      "Fire: FIRE INVESTIGATION                1000\n",
      "EMS: OVERDOSE                           1000\n",
      "EMS: UNCONSCIOUS SUBJECT                1000\n",
      "EMS: CHOKING                            1000\n",
      "EMS: LACERATIONS                        1000\n",
      "Fire: TRASH/DUMPSTER FIRE               1000\n",
      "Fire: UNKNOWN TYPE FIRE                 1000\n",
      "Fire: BUILDING FIRE                     1000\n",
      "Fire: ELECTRICAL FIRE OUTSIDE           1000\n",
      "EMS: FEVER                              1000\n",
      "EMS: ALLERGIC REACTION                  1000\n",
      "EMS: FRACTURE                           1000\n",
      "Fire: WOODS/FIELD FIRE                  1000\n",
      "Fire: FIRE SPECIAL SERVICE              1000\n",
      "Fire: VEHICLE FIRE                      1000\n",
      "Traffic: VEHICLE FIRE -                 1000\n",
      "EMS: EMS SPECIAL SERVICE                1000\n",
      "Traffic: HAZARDOUS ROAD CONDITIONS -    1000\n",
      "EMS: DEHYDRATION                        1000\n",
      "EMS: BUILDING FIRE                      1000\n",
      "Fire: FIRE POLICE NEEDED                1000\n",
      "EMS: CARDIAC ARREST                     1000\n",
      "EMS: MEDICAL ALERT ALARM                1000\n",
      "EMS: SEIZURES                           1000\n",
      "EMS: ASSAULT VICTIM                     1000\n",
      "Fire: APPLIANCE FIRE                    1000\n",
      "Fire: GAS-ODOR/LEAK                     1000\n",
      "EMS: CARDIAC EMERGENCY                  1000\n",
      "EMS: DIZZINESS                          1000\n",
      "EMS: HEAD INJURY                        1000\n",
      "EMS: NAUSEA/VOMITING                    1000\n",
      "EMS: RESPIRATORY EMERGENCY              1000\n",
      "EMS: SYNCOPAL EPISODE                   1000\n",
      "Traffic: VEHICLE ACCIDENT -             1000\n",
      "EMS: VEHICLE ACCIDENT                   1000\n",
      "Traffic: DISABLED VEHICLE -             1000\n",
      "EMS: GENERAL WEAKNESS                   1000\n",
      "EMS: FALL VICTIM                        1000\n",
      "Fire: CARBON MONOXIDE DETECTOR          1000\n",
      "EMS: UNKNOWN MEDICAL EMERGENCY          1000\n",
      "EMS: UNRESPONSIVE SUBJECT               1000\n",
      "Fire: VEHICLE ACCIDENT                  1000\n",
      "EMS: ALTERED MENTAL STATUS              1000\n",
      "Fire: FIRE ALARM                        1000\n",
      "EMS: CVA/STROKE                         1000\n",
      "Traffic: ROAD OBSTRUCTION -             1000\n",
      "EMS: SUBJECT IN PAIN                    1000\n",
      "EMS: HEMORRHAGING                       1000\n",
      "Fire: CARDIAC ARREST                    1000\n",
      "Name: count, dtype: int64\n",
      "Balanced class count: {52}\n"
     ]
    }
   ],
   "source": [
    "# Define maximum number of samples per class\n",
    "amount_per_class = 1000  # You can change this\n",
    "\n",
    "# Empty DataFrame to collect balanced data\n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each unique class in the 'title' column\n",
    "for label in df['title'].unique():\n",
    "    class_subset = df[df['title'] == label]\n",
    "    count = len(class_subset)\n",
    "    \n",
    "    # If the class has fewer samples than 'amount_per_class', skip it\n",
    "    if count < amount_per_class:\n",
    "        continue\n",
    "    \n",
    "    # If the class has more samples than 'amount_per_class', truncate it\n",
    "    if count > amount_per_class:\n",
    "        class_subset = class_subset.sample(amount_per_class)\n",
    "    \n",
    "    balanced_df = pd.concat([balanced_df, class_subset], axis=0)\n",
    "\n",
    "# Reset index for the new balanced DataFrame\n",
    "balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Optionally: Save balanced DataFrame to new CSV\n",
    "# balanced_df.to_csv('balanced_data.csv', index=False)\n",
    "\n",
    "print(\"Balanced DataFrame:\", balanced_df['title'].value_counts())\n",
    "print('Balanced class count:', {len(balanced_df['title'].unique())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and it has a column 'title'\n",
    "# Drop NaN values\n",
    "balanced_df.dropna(subset=['title'], inplace=True)\n",
    "\n",
    "# Lowercase and remove punctuations\n",
    "balanced_df['PreprocessedTitle'] = balanced_df['title'].str.lower().str.replace('[^\\w\\s]', '')\n",
    "labels = balanced_df['PreprocessedTitle'].astype('category').cat.codes.tolist()\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(balanced_df['PreprocessedTitle'], labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(set(labels)))\n",
    "\n",
    "# Tokenize your text\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset\n",
    "class EmergencyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize datasets\n",
    "train_dataset = EmergencyDataset(train_encodings, y_train)\n",
    "val_dataset = EmergencyDataset(val_encodings, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7800' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7800/7800 04:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7800, training_loss=0.12550538304046943, metrics={'train_runtime': 284.3727, 'train_samples_per_second': 438.861, 'train_steps_per_second': 27.429, 'total_flos': 577458999936000.0, 'train_loss': 0.12550538304046943, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "#trainer.train(resume_from_checkpoint=\"./results/checkpoint-500\")\n",
    "# Load the model from a checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./results/checkpoint-7500')  # Replace with your checkpoint directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7586' max='7800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7586/7800 00:03 < 00:08, 24.78 it/s, Epoch 2.92/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./results/checkpoint-7500\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/transformers/trainer.py:1815\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1815\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1816\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/data_loader.py:393\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    394\u001b[0m     next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 160\u001b[0m         {\n\u001b[1;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39mnon_blocking, skip_keys\u001b[39m=\u001b[39mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/utils/operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m    160\u001b[0m         {\n\u001b[0;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ERP/lib/python3.8/site-packages/accelerate/utils/operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[1;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"./results/checkpoint-7500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EMS: BACK PAINS/INJURY' 'EMS: DIABETIC EMERGENCY' 'Fire: GAS-ODOR/LEAK'\n",
      " 'EMS: CARDIAC EMERGENCY' 'EMS: DIZZINESS' 'EMS: HEAD INJURY'\n",
      " 'EMS: NAUSEA/VOMITING' 'EMS: RESPIRATORY EMERGENCY'\n",
      " 'EMS: SYNCOPAL EPISODE' 'Traffic: VEHICLE ACCIDENT -'\n",
      " 'EMS: VEHICLE ACCIDENT' 'Traffic: DISABLED VEHICLE -'\n",
      " 'Fire: APPLIANCE FIRE' 'EMS: GENERAL WEAKNESS'\n",
      " 'Fire: CARBON MONOXIDE DETECTOR' 'EMS: UNKNOWN MEDICAL EMERGENCY'\n",
      " 'EMS: UNRESPONSIVE SUBJECT' 'Fire: VEHICLE ACCIDENT'\n",
      " 'EMS: ALTERED MENTAL STATUS' 'Fire: FIRE ALARM' 'EMS: CVA/STROKE'\n",
      " 'Traffic: ROAD OBSTRUCTION -' 'EMS: SUBJECT IN PAIN' 'EMS: HEMORRHAGING'\n",
      " 'EMS: FALL VICTIM' 'EMS: ASSAULT VICTIM' 'EMS: SEIZURES'\n",
      " 'EMS: MEDICAL ALERT ALARM' 'EMS: ABDOMINAL PAINS'\n",
      " 'Fire: FIRE INVESTIGATION' 'EMS: OVERDOSE' 'EMS: UNCONSCIOUS SUBJECT'\n",
      " 'EMS: CHOKING' 'EMS: LACERATIONS' 'Fire: TRASH/DUMPSTER FIRE'\n",
      " 'Fire: UNKNOWN TYPE FIRE' 'Fire: BUILDING FIRE'\n",
      " 'Fire: ELECTRICAL FIRE OUTSIDE' 'EMS: FEVER' 'EMS: ALLERGIC REACTION'\n",
      " 'EMS: FRACTURE' 'Fire: WOODS/FIELD FIRE' 'Fire: FIRE SPECIAL SERVICE'\n",
      " 'Fire: VEHICLE FIRE' 'Traffic: VEHICLE FIRE -' 'EMS: EMS SPECIAL SERVICE'\n",
      " 'Traffic: HAZARDOUS ROAD CONDITIONS -' 'EMS: DEHYDRATION'\n",
      " 'EMS: BUILDING FIRE' 'Fire: FIRE POLICE NEEDED' 'EMS: CARDIAC ARREST'\n",
      " 'Fire: CARDIAC ARREST']\n"
     ]
    }
   ],
   "source": [
    "print(balanced_df['title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_46', 'score': 0.9980169534683228}]\n",
      "fire: woods/field fire\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "labels = balanced_df['PreprocessedTitle'].astype('category')\n",
    "label_codes = labels.cat.codes.tolist()\n",
    "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "test_string = 'theres a fire in that forest area '\n",
    "\n",
    "print(nlp(test_string))\n",
    "output = nlp(test_string)\n",
    "# Extract the index (84 in this case)\n",
    "label_index = int(output[0]['label'].split('_')[1])\n",
    "original_class_name = labels.cat.categories[label_index]\n",
    "print(original_class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ERP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
